#Lab 1:
elif [[ "$PROVIDER" == "mock" ]]; then
  echo "== LLM Demo (Mock Mode, runs locally) =="
  echo "Prompt: $PROMPT"
  echo "----------------------------------------"

  # Fake "AI response" â€“ you can edit this as you like
  echo "ðŸ¤– Mock LLM says: Based on your input, hereâ€™s a placeholder response."
  echo "This shows how the script would display an AI answer."

  echo "------------------"
  echo "Latency: 5 ms (simulated)"



#Lab 2 :

#!/usr/bin/env bash
# Simple terminal demo of "how an LLM works" with offline mock mode.
# Supports: mock (offline), vertex (Gemini), openai (Responses API)
# Deps: bash, curl, jq (optional), gcloud (only for vertex)

set -euo pipefail

# ----- Defaults -----
PROVIDER="${PROVIDER:-mock}"   # mock | vertex | openai
PROMPT="${1:-"Explain like I'm five: What is CI/CD?"}"
TEMPERATURE="${TEMPERATURE:-0.2}"
TOP_P="${TOP_P:-1.0}"
MAX_TOKENS="${MAX_TOKENS:-256}"

# Small helper for a clean header
print_header () {
  echo "== LLM Demo (${1}) =="
  echo "Prompt: $PROMPT"
  echo "Params: temperature=$TEMPERATURE | top_p=$TOP_P | max_tokens=$MAX_TOKENS"
  echo "----------------------------------------"
}

if [[ "$PROVIDER" == "mock" ]]; then
  # ----------------- MOCK MODE (offline) -----------------
  print_header "Mock Mode (offline)"
  START=$(date +%s%3N)

  # A tiny "toy model" just to show behavior differences with parameters
  # - If temperature > 0.5, add a creative flourish.
  # - If top_p < 1.0, pretend to be more concise.
  # This is purely illustrative!
  RESPONSE="I'm a local mock LLM. Here's a simple explanation:\n\nCI/CD is a way to automatically build, test, and deliver software so changes reach users faster and safer."

  awk '
    BEGIN {
      temp = ENVIRON["TEMPERATURE"] + 0.0
      topp = ENVIRON["TOP_P"] + 0.0
      resp = ENVIRON["RESPONSE"]
      prompt = ENVIRON["PROMPT"]
    }
    function print_creative() {
      print "\n(creative note) Think of it like a bakery: code is dough, tests are ovens, and CD is the delivery truck!"
    }
    function print_concise() {
      print "\n(concise mode) CI checks code; CD ships it."
    }
    END {
      print resp
      if (temp > 0.5) { print_creative() }
      if (topp < 1.0) { print_concise() }
      print "\n(mock) I didnâ€™t call any external API."
    }
  ' >/tmp/mock_out.txt

  cat /tmp/mock_out.txt
  END=$(date +%s%3N)
  echo "------------------"
  echo "Latency: $((END-START)) ms (simulated)"
  exit 0
fi

# ----------------- REAL PROVIDERS BELOW -----------------

if [[ "$PROVIDER" == "vertex" ]]; then
  MODEL="${MODEL:-gemini-1.5-flash}"
  GCP_PROJECT="${GCP_PROJECT:-}"
  GCP_LOCATION="${GCP_LOCATION:-us-central1}"

  if ! command -v gcloud >/dev/null 2>&1; then
    echo "ERROR: gcloud CLI not found. Install and run 'gcloud auth application-default login'." >&2
    exit 1
  fi
  if [[ -z "${GCP_PROJECT}" ]]; then
    echo "ERROR: set GCP_PROJECT env var (your GCP project ID)." >&2
    exit 1
  fi

  print_header "Vertex AI (Gemini: $MODEL)"
  ACCESS_TOKEN="$(gcloud auth application-default print-access-token)"

  START=$(date +%s%3N)
  RESP="$(
    curl -sS -X POST \
      "https://$GCP_LOCATION-aiplatform.googleapis.com/v1/projects/$GCP_PROJECT/locations/$GCP_LOCATION/publishers/google/models/$MODEL:generateContent" \
      -H "Authorization: Bearer $ACCESS_TOKEN" \
      -H "Content-Type: application/json; charset=utf-8" \
      -d @- <<EOF
{
  "contents": [{
    "role": "user",
    "parts": [{"text": "$PROMPT"}]
  }],
  "generationConfig": {
    "temperature": $TEMPERATURE,
    "topP": $TOP_P,
    "maxOutputTokens": $MAX_TOKENS
  }
}
EOF
  )"
  END=$(date +%s%3N)

  echo
  echo "---- Response ----"
  if command -v jq >/dev/null 2>&1; then
    echo "$RESP" | jq -r '.candidates[0].content.parts[0].text // .error.message // "No text content"'
  else
    echo "$RESP"
  fi
  echo "------------------"
  echo "Latency: $((END-START)) ms"
  exit 0

elif [[ "$PROVIDER" == "openai" ]]; then
  MODEL="${MODEL:-gpt-4o-mini}"
  if [[ -z "${OPENAI_API_KEY:-}" ]]; then
    echo "ERROR: set OPENAI_API_KEY env var." >&2
    exit 1
  fi

  print_header "OpenAI ($MODEL)"

  START=$(date +%s%3N)
  RESP="$(
    curl -sS -X POST https://api.openai.com/v1/responses \
      -H "Authorization: Bearer $OPENAI_API_KEY" \
      -H "Content-Type: application/json" \
      -d @- <<EOF
{
  "model": "$MODEL",
  "input": [{"role":"user","content":"$PROMPT"}],
  "temperature": $TEMPERATURE,
  "top_p": $TOP_P,
  "max_output_tokens": $MAX_TOKENS
}
EOF
  )"
  END=$(date +%s%3N)

  echo
  echo "---- Response ----"
  if command -v jq >/dev/null 2>&1; then
    # Try different fields the Responses API may return
    echo "$RESP" | jq -r '.output[0].content[0].text // .output_text // .error.message // "No text content"'
  else
    echo "$RESP"
  fi
  echo "------------------"
  echo "Latency: $((END-START)) ms"
  exit 0

else
  echo "ERROR: PROVIDER must be 'mock', 'vertex', or 'openai'." >&2
  exit 1
fi

